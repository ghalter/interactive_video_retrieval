{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Network for Image Captioning\n",
    "\n",
    "In this notebook we will train the image captioning model, which we will later use for the scene-thumbnails of our own dataset. In the following, the training of a Encoder-Decoder Network with a LSTM-RNN will be explained step-by-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycocotools in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.5)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from nltk) (2020.4.4)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from nltk) (0.13.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from nltk) (4.45.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from nltk) (7.1.1)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "import torch\n",
    "import torchtext.vocab as vocab\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "GPU device not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ca5fc7c77d54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'/device:GPU:0'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU device not found'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found GPU at: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemError\u001b[0m: GPU device not found"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is CPU\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "if str(device) == \"cuda\":\n",
    "    torch.cuda.get_device_name(0)\n",
    "    print(\"device is GPU\")\n",
    "\n",
    "else:\n",
    "    print(\"device is CPU\")\n",
    "\n",
    "cudnn.benchmark = True              # set to true only if inputs to model are fixed size; otherwise lot of computational overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration for Data-Processing, Models and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class Arguments(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # Dataset Processing Parameters\n",
    "        self.environment = 'local'            # Location of data\n",
    "        self.vocab_cutoff = 15                 # Cutoff value for words to get loaded to vocabulary\n",
    "        self.vocab = True                     # False --> Create vocabulary, True --> Load vocabulary\n",
    "        self.do_vectorize = True              # True --> Vectorize training set\n",
    "        self.cpi  = 5                         # CPI = number of annotations per image\n",
    "        \n",
    "        # Files\n",
    "        self.dir_path = os.path.dirname(os.path.realpath('Image Captioning'))\n",
    "        self.annotations_path = '/dataset/files/annotations/'\n",
    "        self.data_path = '/Volumes/Samsung_X5/University/Semesters/Semester 6/Interactive Video Retrieval/'\n",
    "        self.thumbnails = '/Volumes/Samsung_X5/University/Semesters/Semester 6/Interactive Video Retrieval/thumbnails/'\n",
    "        self.vocab_path = self.dir_path + '/vocab/vocab_cut15.pkl'\n",
    "        self.caps_token_path = self.dir_path + '/caps_token.pkl'\n",
    "        self.caps_len_path = self.dir_path + '/caps_len.pkl'\n",
    "        self.pretrained_model = '/Volumes/Samsung_X5/University/Semesters/Semester 6/Interactive Video Retrieval/Model/checkpoint_img_captioning_coco.pth.tar'\n",
    "        \n",
    "        # Training Parameters\n",
    "        self.batch_size = 100\n",
    "        self.seed = 42\n",
    "        self.start_epoch = 0\n",
    "        self.epochs = 100                   # number of epochs to train for (if early stopping is not triggered)\n",
    "        self.epochs_since_improvement = 0   # keeps track of number of epochs since there's been an improvement in validation BLEU\n",
    "        self.total_step =  600              # Number of steps per epoch (training)\n",
    "        self.test_total_step = 200          # Number of steps in testing\n",
    "        self.workers = 1                    # for data-loading;\n",
    "        self.encoder_lr = 1e-4              # learning rate for encoder if fine-tuning\n",
    "        self.decoder_lr = 0.00035           # learning rate for decoder\n",
    "        self.grad_clip = 5.                 # clip gradients at an absolute value of\n",
    "        self.alpha_c = 1.                   # regularization parameter for 'doubly stochastic attention', as in the paper\n",
    "        self.best_bleu = 0.215371729        # BLEU-4 score right now\n",
    "        self.print_freq = 100               # print training/validation stats every __ batches\n",
    "        self.fine_tune_encoder = True       # fine-tune encoder?\n",
    "        self.checkpoint = '/Volumes/Samsung_X5/University/Semesters/Semester 6/Interactive Video Retrieval/Model/checkpoint_img_captioning_coco.pth.tar' # path to checkpoint, None if none\n",
    "        self.model_name = 'img_captioning_coco' # model name for saving a checkpoing\n",
    "    \n",
    "       \n",
    "        # Model Parameters\n",
    "        self.emb_dim = 300                  # dimension of word embeddings. Depends on pre-trained model!!!\n",
    "        self.attention_dim = 512            # dimension of attention linear layers\n",
    "        self.decoder_dim = 512              # dimension of decoder RNN\n",
    "        self.dropout = 0.5\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 399999/400000 [00:42<00:00, 9425.00it/s]\n"
     ]
    }
   ],
   "source": [
    "glove = vocab.GloVe(name='6B', dim=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocess the dataset\n",
    "First of all, let's get a little bit familiar with the dataset. You can either download the dataset (about 14GB for training and 6GB for validation) or retrieve the images using the URL, which can be found in the files instances_train2014.json or instances_test2014. \n",
    "\n",
    "#### DataPreprocess\n",
    "The dataset is comprised by about 600.000 images in the training dataset, each annotated with 5 captions. We use the class DataPreprocess to load all image names, which are used to construct the path to the image, as well as the corresponding annotation IDs. Once we have extracted them, we load all captions in plain text and process the text. We a) construct a vocublary of all words that appear more often than a certain cutoff value (= 5) and b) tokenize all captions in the training set and test (=validation) set using the word-index in the vocabulary. You can also use a pre-extracted vocabulary and load it to save time! Before tokenizing, we add a start ([BOS]) and end ([EOS]) token to the captions. Even though vectorizing and creation of the vocabulary are not done in this class, the DataPreprocess instantiates these classes and executes it. \n",
    "<br> <br>\n",
    "**_Simply call and parse the arguments-file:_**\n",
    "<br> data_object = DataPreprocess.ProcessData(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "\n",
    "class DataPreprocess(object):\n",
    "\n",
    "    \n",
    "    def __init__(self, train_img_names, train_cap_tokens, train_caps, train_caps_len,\n",
    "                 test_img_names, test_cap_tokens, test_caps, test_caps_len, \n",
    "                 train_indices, test_indices,\n",
    "                 vectorizer, vocab):\n",
    "        \n",
    "        # Training and test data\n",
    "        self.train_img_names = train_img_names\n",
    "        self.train_cap_tokens = train_cap_tokens\n",
    "        self.train_caps = train_caps\n",
    "        self.train_caps_len = train_caps_len\n",
    "        \n",
    "        self.test_img_names = test_img_names\n",
    "        self.test_cap_tokens = test_cap_tokens\n",
    "        self.test_caps = test_caps\n",
    "        self.test_caps_len = test_caps_len\n",
    "        \n",
    "        # Indices\n",
    "        self.train_indices = train_indices\n",
    "        self.test_indices = test_indices\n",
    "        \n",
    "        # Utils\n",
    "        self.vectorizer = vectorizer\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        \n",
    "    @classmethod\n",
    "    def ProcessData(cls, args):\n",
    "        '''\n",
    "        This is the main method in the DataPreprocess-class. Call this function to instantiate the class and\n",
    "        to preprocess the dataset. It comprises:\n",
    "        - load of the coco-IDs to extract the image names and corresponding annotations with LoadCoco()\n",
    "        - load of the corresponding captions and the image names into lists with LoadFiles() \n",
    "        - instantiate of the vecotrizer and vocabulary class with ProcessCaptions()\n",
    "        - vectorize/ tokenize the annotations of the training and test sets\n",
    "        \n",
    "        Return: train_img_names, train_cap_tokens, train_caps, train_caps_len, test_img_names,\n",
    "                   test_cap_tokens, test_caps, test_caps_len, vectorizer, vocab\n",
    "        '''\n",
    "        \n",
    "        train_dataType = 'train2014'\n",
    "        test_dataType = 'val2014'\n",
    "            \n",
    "        # Load the coco-instances that include the img_ids, the coco_instance and the coco_captions\n",
    "        train_img_ids, train_coco_inst, train_coco_caps = DataPreprocess.LoadCoco(train_dataType, args)\n",
    "        test_img_ids, test_coco_inst, test_coco_caps = DataPreprocess.LoadCoco(test_dataType, args)\n",
    "        \n",
    "        # Load coco_ids and file_names of every image in the dataset\n",
    "        train_img_coco_ids, train_img_names, train_cap_coco_ids, train_caps = DataPreprocess.LoadFiles(train_img_ids, train_coco_inst, train_coco_caps)\n",
    "        test_img_coco_ids, test_img_names, test_cap_coco_ids, test_caps = DataPreprocess.LoadFiles(test_img_ids, test_coco_inst, test_coco_caps)\n",
    "        \n",
    "        # Process the captions, i.e. load the vocab and tokenize each word (only trainin data)\n",
    "        vectorizer, vocab, train_cap_tokens, train_caps_len = DataPreprocess.ProcessCaptions(train_caps, args)\n",
    "        \n",
    "        # Vectorize test set\n",
    "        test_cap_tokens, test_caps_len = vectorizer.vectorize(test_caps)\n",
    "        \n",
    "        # Get indices for training and test set\n",
    "        train_indices = DataPreprocess.GetIndices(train_img_names, train_cap_tokens, train_caps, args)\n",
    "        test_indices = DataPreprocess.GetIndices(test_img_names, test_cap_tokens, test_caps, args)\n",
    "        \n",
    "       \n",
    "        return cls(train_img_names, train_cap_tokens, train_caps, train_caps_len, test_img_names,\n",
    "                   test_cap_tokens, test_caps, test_caps_len,\n",
    "                   train_indices, test_indices, \n",
    "                   vectorizer, vocab)\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def LoadCoco(dataType, args):\n",
    "        \n",
    "        # initialize COCO API for instance annotations\n",
    "        dataDir = args.annotations_path\n",
    "        instances_annFile = os.path.join(dataDir, 'instances_{}.json'.format(dataType))\n",
    "        coco_inst = COCO(instances_annFile)\n",
    "\n",
    "        # initialize COCO API for caption annotations\n",
    "        captions_annFile = os.path.join(dataDir, 'captions_{}.json'.format(dataType))\n",
    "        coco_caps = COCO(captions_annFile)\n",
    "\n",
    "        # get image ids \n",
    "        img_ids = list(coco_inst.anns.keys())\n",
    "              \n",
    "        return img_ids, coco_inst, coco_caps\n",
    "        \n",
    "    \n",
    "    # So far, the coco_files continue a lot of unnecessary information. Extract only ids and file_names\n",
    "    @staticmethod\n",
    "    def LoadFiles(img_ids, coco_inst, coco_caps):\n",
    "        \n",
    "        # To store image_names and their ids in the coco files\n",
    "        img_coco_ids = []\n",
    "        img_names = []\n",
    "        \n",
    "        # Load captions (caps) and their ids in the coco files\n",
    "        caps_coco_ids = []\n",
    "        caps = []\n",
    "\n",
    "        for i in img_ids:\n",
    "            img_id = coco_inst.anns[i]['image_id']\n",
    "            img_name = coco_inst.loadImgs(img_id)[0]\n",
    "    \n",
    "            img_coco_ids.append(img_name['id'])\n",
    "            img_names.append(img_name['file_name'])\n",
    "            \n",
    "    \n",
    "        for image in img_coco_ids:\n",
    "            annIds = coco_caps.getAnnIds(image)\n",
    "            caps_coco_ids.append(annIds)\n",
    "\n",
    "            anns = coco_caps.loadAnns(annIds)\n",
    "            annotations = []\n",
    "\n",
    "            for annotation in anns:\n",
    "                annotations.append(annotation['caption'])\n",
    "\n",
    "            caps.append(annotations)\n",
    "            \n",
    "        return img_coco_ids, img_names, caps_coco_ids, caps\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def ProcessCaptions(caps, args):\n",
    "        \n",
    "        vectorizer = Vectorizer.CreateVectorizer(caps, args)\n",
    "        \n",
    "        if args.do_vectorize == True:\n",
    "            \n",
    "            # Tokenize captions and add start/end token to caption\n",
    "            caps_token, caps_len = vectorizer.vectorize(caps)\n",
    "            \n",
    "            # Save tokenized captions and their length\n",
    "            dir_path = args.dir_path\n",
    "            tokenized_caps = '/train_caps_token.pkl'\n",
    "            length_caps = '/train_caps_len.pkl'\n",
    "            \n",
    "            path_token = dir_path + tokenized_caps\n",
    "            path_length = dir_path + length_caps\n",
    "        \n",
    "            with open(path_token, 'wb') as f:\n",
    "                pickle.dump(caps_token, f)\n",
    "                \n",
    "            with open(path_length, 'wb') as f:\n",
    "                pickle.dump(caps_len, f)\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            # Load pre-tokenized captions and their lengths\n",
    "            file = open(args.caps_token_path, 'rb')\n",
    "            caps_token = pickle.load(file)\n",
    "            file.close()\n",
    "            \n",
    "            file = open(args.caps_len_path, 'rb')\n",
    "            caps_len = pickle.load(file)\n",
    "            file.close()\n",
    "            \n",
    "            \n",
    "        return vectorizer, vectorizer.vocab, caps_token, caps_len\n",
    "            \n",
    "            \n",
    "    @staticmethod\n",
    "    def GetIndices(img_names, cap_tokens, caps, args):\n",
    "        \n",
    "        print(\"Start getting indices...\")\n",
    "        \n",
    "        data = []  # list to store [[cap_idx, cap, img_name, ref_caps],...]\n",
    "        counter = 0  \n",
    "\n",
    "        # Tranfer img_names into dimensionality of cap_tokens using the cpi\n",
    "        for i, img_name in enumerate(img_names):\n",
    "\n",
    "            reference_token = []\n",
    "\n",
    "            # Get all captions for image and save their ids\n",
    "            for j in range(len(caps[i])):\n",
    "                reference_indices = counter + j\n",
    "                reference_token.append(reference_indices)\n",
    "\n",
    "            # Add each token id, the corresponding image and the corresponding captions\n",
    "            for k in range(len(caps[i])):\n",
    "                cap_index = counter +k\n",
    "                cap_text = (cap_tokens[cap_index])\n",
    "                data.append([cap_index, cap_tokens[cap_index], img_name, reference_token])\n",
    "\n",
    "            # Increase counter by the length of the captions per image\n",
    "            counter += len(reference_token)\n",
    "\n",
    "        # Save all indices into a pd.DataFrame\n",
    "        indices = pd.DataFrame(data, columns = ['Caption Index', 'Tokenized Caption', 'Image Name', 'Reference Captions'])\n",
    "\n",
    "        return indices\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizer Class\n",
    "The _Vectorizer Class_ load all captions as plain text, iterates over all captions to extract every word, extracts only the words beyond a certain cutoff-threshold (set in the arguments-class), and builds the vocabulary. Once the vocabulary is build, the _Vectorizer Class_ is used to tokenize the words in each annotation/ caption using the index of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "class Vectorizer(object):\n",
    "    \n",
    "    def __init__(self, vocab, args):\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        self.args = args\n",
    "        \n",
    "    \n",
    "    @classmethod\n",
    "    def CreateVectorizer(cls, caps, args):\n",
    "        \"\"\" Instantiates the vectorizer from the dataset frame\n",
    "        Arguments:\n",
    "            review_df = Pandas DataFrame passed from class ReviewDataset\n",
    "            cutoff = parameter for frequency based filtering\n",
    "        Returns:\n",
    "            an instance of the class ReviewVectorizer\n",
    "        \"\"\"\n",
    "        \n",
    "        # If vocab exists, load vocab\n",
    "        if args.vocab == True:\n",
    "            \n",
    "            print(\"Load existing vocabulary\")\n",
    "            \n",
    "            file = open(args.vocab_path, 'rb')\n",
    "            vocab = pickle.load(file)\n",
    "            file.close()\n",
    "            \n",
    "        \n",
    "        # If vocab doesn't exist, create new vocab\n",
    "        else:\n",
    "        \n",
    "            print(\"Create new vocabulary\")\n",
    "            # Instantiate new vocabulary\n",
    "            vocab = Vocabulary(add_unk = True)\n",
    "\n",
    "            # Retrieve all words from the captions-file and lower them \n",
    "            all_words = []\n",
    "\n",
    "            for cap in caps:\n",
    "                for annotation in cap:\n",
    "                    \n",
    "                    # Remove punctuation and lower the words\n",
    "                    annotation = annotation[:(len(annotation)-1)]\n",
    "                    annotation = annotation.lower()\n",
    "                    annotation = annotation.replace('@','')\n",
    "                    annotation = annotation.replace('/','')\n",
    "                    annotation = annotation.replace('#','')\n",
    "                    annotation = annotation.replace('_','')\n",
    "                    annotation = annotation.replace('|','')\n",
    "                    annotation = annotation.replace(\"'\",'')\n",
    "                    annotation = annotation.replace('.','')\n",
    "\n",
    "                    words = annotation.split()\n",
    "                    all_words.extend(words)\n",
    "\n",
    "            # Convert list with words to dataframe\n",
    "            all_words_df = pd.DataFrame(all_words)\n",
    "\n",
    "            # This returns a dataframe series with the counts of every single words\n",
    "            single_words_counts = all_words_df.apply(pd.Series.value_counts)\n",
    "            single_words_list = single_words_counts.index.tolist()\n",
    "\n",
    "            # Add words to vocab that occur more often than cutoff value\n",
    "            print(\"Start adding words to vocab\")\n",
    "            for words in single_words_list:\n",
    "\n",
    "                if single_words_counts.loc[words].values > args.vocab_cutoff:\n",
    "                    vocab.add_token(words)\n",
    "            \n",
    "            # Add start and end token to the vocab\n",
    "            vocab.add_token('[BOS]')\n",
    "            vocab.add_token('[EOS]')\n",
    "            vocab.add_token('[PAD]')\n",
    "            \n",
    "            # Save vocab for later usage\n",
    "            dir_path = args.dir_path\n",
    "            file_name = '/vocab.pkl'\n",
    "            path = dir_path + file_name\n",
    "        \n",
    "            with open(path, 'wb') as f:\n",
    "                pickle.dump(vocab, f)\n",
    "        \n",
    "            \n",
    "        return cls(vocab, args)\n",
    "    \n",
    "    \n",
    "    # Not tested yet\n",
    "    def vectorize(self, caps, MAX_LEN = 128):\n",
    "        \n",
    "        print(\"Start Tokenization\")\n",
    "        \n",
    "        # Add start and end token to the vocab\n",
    "        caps_tokens = []\n",
    "        caps_tokens_flat = []\n",
    "        caps_length  = []\n",
    "        caps_length_flat = []\n",
    "        \n",
    "        start_token = self.vocab.lookup_token('[BOS]')\n",
    "        end_token = self.vocab.lookup_token('[EOS]')\n",
    "        pad_token = self.vocab.lookup_token('[PAD]')\n",
    "        \n",
    "        for cap in caps:\n",
    "            \n",
    "            cap_tokens = []\n",
    "            cap_length = []\n",
    "            \n",
    "            for annotation in cap:\n",
    "                \n",
    "                annotation_token = []\n",
    "                \n",
    "                # Remove punctuation and lower the words\n",
    "                annotation = annotation[:(len(annotation)-1)]\n",
    "                annotation = annotation.lower()\n",
    "                words = annotation.split()\n",
    "                \n",
    "                # Add start token to each caption\n",
    "                annotation_token.append(start_token)\n",
    "            \n",
    "                # For each word in the caption, retrieve the token\n",
    "                for word in words:\n",
    "                    annotation_token.append(self.vocab.lookup_token(word))\n",
    "                 \n",
    "                # Add end token to each caption\n",
    "                annotation_token.append(end_token)\n",
    "                cap_length.append(len(annotation_token))\n",
    "                caps_length_flat.append(len(annotation_token))\n",
    "                \n",
    "                for i in range(MAX_LEN - len(annotation_token)):\n",
    "                    annotation_token.append(pad_token)\n",
    "                \n",
    "                # Add tokens to array (looks like [img1[[cap11],[cap12],[cap13]], img2[[cap21]...]...])\n",
    "                cap_tokens.append(annotation_token)\n",
    "                \n",
    "                # Add tokens to flat_array (looks like [[cap11],[cap12],[cap13],[cap21]...])\n",
    "                caps_tokens_flat.append(annotation_token)\n",
    "                \n",
    "            caps_tokens.append(cap_tokens)\n",
    "            caps_length.append(cap_length)\n",
    "        \n",
    "        return caps_tokens_flat, caps_length_flat \n",
    "    \n",
    "    \n",
    "    # Updates vocab and saves new vocab file\n",
    "    def update_vocab(self, word):\n",
    "        \n",
    "        self.vocab.add_token(word)\n",
    "        vocab = self.vocab\n",
    "        \n",
    "        # Save vocab\n",
    "        dir_path = self.args.dir_path\n",
    "        file_name = '/vocab.pkl'\n",
    "        path = dir_path + file_name\n",
    "        \n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(vocab, f)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabulary Class\n",
    "The _Vocabulary Class_ is typically called by the _Vectorizer Class_. With the functions _add_token()_ and _lookup_token()_ you can add and retrieve the index of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \n",
    "    def __init__(self, token_to_idx = None, add_unk = True, unk_token = \"[UNK]\"):\n",
    "        \n",
    "        if token_to_idx is None: # if there doesn't preexist a map of tokens, create a new index\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        \n",
    "        self._idx_to_token = {idx: token\n",
    "                             for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        \n",
    "        self.unk_index = -1 # if we don't have unkown token, then unk_index = -1\n",
    "        if add_unk:  # if token is unkown, pass it to add_token and get the indes for the <unk> token\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "        \n",
    "    \n",
    "    def add_token(self, token):\n",
    "        \"\"\" Update mapping dictionariy based on the token\n",
    "        Arguments:\n",
    "            token = string/ word that should be inserted to dictionary\n",
    "        Returns:\n",
    "            index(int) = corresponding int-index for token\n",
    "        \"\"\"\n",
    "        \n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "            \n",
    "        return index\n",
    "        \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\" Retrieves the index of a token or in case that no token is present the index of 'UNK'\n",
    "        Arguments:\n",
    "            token(str) = the token for which the index should be retrieved\n",
    "        Return:\n",
    "            index(int) = the index associated with the token\n",
    "        \"\"\"\n",
    "        if self._add_unk:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx(token)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Dataset using Dataloader\n",
    "Now that we have pre-processed the annotations/ captions and have loaded the names of all images using the DataPreprocess class, we can build our dataset. Unlike in different settings, in which we load the entire training set into the dataloader, it is more feasible to load the images batch-by-batch. For this, we will use two classes: <br> \n",
    "1. **_LoadData_**: takes an instance of _DataPreprocess()_ to load the image names and the tokenized captions. Using the function _CreateDataLoader()_, we call the class _CocoDataset()_ to instantiate a dataset object. This is then parsed to PyTorch's DataLoader. We sample the instances, i.e. images and captions using their index, randomly based on the length of the caption (function _Get_Train_Indices()_).\n",
    "2. **_CocoDataset_**: is used to actually load the images based on a caption-index (as PIL files), transforms the images and sends an image together with the corresponding-caption to the DataLoader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "\n",
    "class LoadData(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self, data_object, args):\n",
    "        \n",
    "        # Load training and test data\n",
    "        self.train_img_names = data_object.train_img_names\n",
    "        self.test_img_names = data_object.test_img_names\n",
    "        \n",
    "        self.train_cap_tokens = data_object.train_cap_tokens\n",
    "        self.test_cap_tokens = data_object.test_cap_tokens\n",
    "        \n",
    "        self.train_caps_len = data_object.train_caps_len\n",
    "        self.test_caps_len = data_object.test_caps_len\n",
    "        \n",
    "        self.train_indices = data_object.train_indices\n",
    "        self.test_indices = data_object.test_indices\n",
    "        \n",
    "        self.args = args\n",
    "        self.num_workers = 0\n",
    "        self.transform = transforms.Compose([ \n",
    "                transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "                transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "                transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "                transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "                transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                                     (0.229, 0.224, 0.225))])\n",
    "        \n",
    "    \n",
    "    def CreateDataloader(self, mode):\n",
    "        \n",
    "        if mode == 'train':\n",
    "            \n",
    "            batch_size = args.batch_size\n",
    "            indices = LoadData.Get_Random_Train_Indices(self, batch_size, mode)\n",
    "            img_folder = args.data_path + 'train2014/'\n",
    "            indices, img_names, cap_tokens, caps_len, cap_references = LoadData.Get_Data(self, indices, mode)\n",
    "              \n",
    "                \n",
    "        elif mode == 'test':\n",
    "            \n",
    "            batch_size = 1\n",
    "            indices = LoadData.Get_Train_Indices(self, batch_size, mode)\n",
    "            img_folder = args.data_path + 'val2014/'\n",
    "            indices, img_names, cap_tokens, caps_len, cap_references = LoadData.Get_Data(self, indices, mode)\n",
    "            \n",
    "    \n",
    "        # Create new dataset instance, which will be fed into the dataloader\n",
    "        dataset = CoCoDataset(transform = self.transform,\n",
    "                                mode = mode,\n",
    "                                batch_size = batch_size,\n",
    "                                img_path = img_folder,\n",
    "                                img_names =  img_names,\n",
    "                                cap_tokens = cap_tokens,\n",
    "                                caps_len = caps_len,\n",
    "                                cap_references = cap_references,\n",
    "                                args = self.args)\n",
    "\n",
    "\n",
    "        initial_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        \n",
    "        data_loader = data.DataLoader(dataset = dataset, \n",
    "                                      num_workers = self.num_workers,\n",
    "                                      batch_sampler=data.sampler.BatchSampler(sampler=initial_sampler,\n",
    "                                                                              batch_size=dataset.batch_size,\n",
    "                                                                              drop_last=False))\n",
    "        \n",
    "        return data_loader\n",
    "            \n",
    "    \n",
    "    # It is way faster to get random indices and only pass a subset of the data to the dataset instance.\n",
    "    @staticmethod    \n",
    "    def Get_Random_Train_Indices(self, batch_size, mode):\n",
    "        \n",
    "        if mode == 'train':\n",
    "            all_indices = random.sample(range(len(self.train_cap_tokens)), batch_size)\n",
    "        \n",
    "        else:\n",
    "            all_indices = random.sample(range(len(self.test_cap_tokens)), batch_size)\n",
    "            \n",
    "        return all_indices\n",
    "    \n",
    "    \n",
    "    # Just for test purposes\n",
    "    @staticmethod\n",
    "    def Get_Data(self, indices, mode):\n",
    "        \n",
    "        sub_img_names = []\n",
    "        sub_cap_tokens = []\n",
    "        sub_caps_len = []\n",
    "        sub_indices = []\n",
    "        cap_references = []\n",
    "        \n",
    "        for i, index in enumerate(indices):\n",
    "            \n",
    "            try:\n",
    "                if mode == 'train':\n",
    "\n",
    "                    sub_img_names.append(self.train_indices.loc[index, 'Image Name'])\n",
    "                    sub_cap_tokens.append(self.train_cap_tokens[index])\n",
    "                    sub_caps_len.append(self.train_caps_len[index])\n",
    "\n",
    "                    cap_references.append(self.train_indices.loc[index, 'Reference Captions'])\n",
    "\n",
    "                else:\n",
    "\n",
    "                    sub_img_names.append(self.test_indices.loc[index, 'Image Name'])\n",
    "                    sub_cap_tokens.append(self.test_cap_tokens[index])\n",
    "                    sub_caps_len.append(self.test_caps_len[index])\n",
    "\n",
    "                    cap_references.append(self.test_indices.loc[index, 'Reference Captions'])\n",
    "            \n",
    "            except: pass\n",
    "                \n",
    "            sub_indices.append(i)\n",
    "        \n",
    "        return sub_indices, sub_img_names, sub_cap_tokens, sub_caps_len, cap_references\n",
    "       \n",
    "    \n",
    "    #If captions are not padded, we can use this function to derive captions with equal length.\n",
    "    @staticmethod    \n",
    "    def Get_Train_Indices(self, batch_size, mode):\n",
    "        \n",
    "        if mode == 'train':\n",
    "            sel_length = np.random.choice(self.train_caps_len)\n",
    "            all_indices = np.where([self.train_caps_len[i] == sel_length for i in np.arange(len(self.train_caps_len))])[0]\n",
    "        \n",
    "        else:\n",
    "            sel_length = np.random.choice(self.test_caps_len)\n",
    "            all_indices = np.where([self.test_caps_len[i] == sel_length for i in np.arange(len(self.test_caps_len))])[0]\n",
    "            \n",
    "        \n",
    "        indices = list(np.random.choice(all_indices, size=batch_size))\n",
    "        \n",
    "        return indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_CocoDataset()_ is used as Dataset class for the DataLoader. It is called everytime a new batch is retrieved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "class CoCoDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, transform, mode, batch_size, img_path, img_names, cap_tokens, caps_len, cap_references, args):\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        self.batch_size = batch_size\n",
    "        self.img_path = img_path\n",
    "        self.cap_references = cap_references\n",
    "        \n",
    "        self.img_names = img_names\n",
    "        self.cap_tokens = cap_tokens\n",
    "        self.caps_len = caps_len\n",
    "        self.cpi = args.cpi\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        img_idx = index   # 5 captions per image\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            \n",
    "            # Get images\n",
    "            img_name = self.img_names[img_idx]\n",
    "            image_path = self.img_path + img_name\n",
    "\n",
    "            # Convert image to tensor and pre-process using transform\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image = self.transform(image)\n",
    "\n",
    "            # Convert caption to tensor of word ids.\n",
    "            caption = self.cap_tokens[index]\n",
    "            caption = torch.tensor(caption, dtype=torch.long)\n",
    "            caption_len = self.caps_len[index]\n",
    "            caption_len = torch.tensor(caption_len, dtype=torch.long)\n",
    "\n",
    "            # return pre-processed image and caption tensors\n",
    "            return image, caption, caption_len\n",
    "        \n",
    "\n",
    "        # obtain image if in test mode\n",
    "        else:\n",
    "            \n",
    "            # Get images\n",
    "            img_name = self.img_names[img_idx]\n",
    "            image_path = self.img_path + img_name\n",
    "\n",
    "            # Convert image to tensor and pre-process using transform\n",
    "            PIL_image = Image.open(image_path).convert('RGB')\n",
    "            image = self.transform(PIL_image)\n",
    "            \n",
    "            # Convert caption to tensor of word ids.\n",
    "            caption = self.cap_tokens[index]\n",
    "            caption = torch.tensor(caption, dtype=torch.long)\n",
    "            caption_len = self.caps_len[index]\n",
    "            caption_len = torch.tensor(caption_len, dtype=torch.long)\n",
    "            \n",
    "            cap_reference = self.cap_references\n",
    "\n",
    "            # return original image and pre-processed image tensor\n",
    "            return image, caption, caption_len, cap_reference\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Network Architecture\n",
    "The image captioning task follows a **Encoder-Decoder** architecture. The **Encoder** will be a pre-trained **ResNet-101** model to encode the images into an embedded image vector. To retrieve the embedding-vectors, we have to cut-off the last layers of the ResNet-model. The output of the Encoder will be a (1, 2048)-dim vector. <br> <br>\n",
    "This vector is then fed as start-state/hidden value into the **Decoder**, which is a LSTM-RNN with Attention. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](model_figure.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Source: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning/blob/master/models.py\n",
    "'''\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoded_image_size=14):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_image_size = encoded_image_size\n",
    "\n",
    "        resnet = torchvision.models.resnet101(pretrained=True)  # pretrained ImageNet ResNet-101\n",
    "\n",
    "        # Remove linear and pool layers as original ResNet Model is trained on image classification\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "\n",
    "        # Resize image to fixed size to allow input images of variable size\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n",
    "\n",
    "        self.fine_tune()\n",
    "\n",
    "        \n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param images: images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n",
    "        :return: encoded images\n",
    "        \"\"\"\n",
    "        output = self.resnet(images)         # (batch_size, 2048, image_size/32, image_size/32)\n",
    "        output = self.adaptive_pool(output)  # (batch_size, 2048, encoded_image_size, encoded_image_size)\n",
    "        output = output.permute(0, 2, 3, 1)  # (batch_size, encoded_image_size, encoded_image_size, 2048)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    \n",
    "    def fine_tune(self, fine_tune=True):\n",
    "        \"\"\"\n",
    "        Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.\n",
    "        :param fine_tune: Allow?\n",
    "        \"\"\"\n",
    "        for p in self.resnet.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        # If fine-tuning, only fine-tune convolutional blocks 2 through 4\n",
    "        for c in list(self.resnet.children())[5:]:\n",
    "            for p in c.parameters():\n",
    "                p.requires_grad = fine_tune\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        \"\"\"\n",
    "        :param encoder_dim: feature size of encoded images\n",
    "        :param decoder_dim: size of decoder's RNN\n",
    "        :param attention_dim: size of the attention network\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n",
    "        self.full_att = nn.Linear(attention_dim, 1)               # linear layer to calculate values to be softmax-ed\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)                          # softmax layer to calculate weights\n",
    "\n",
    "        \n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
    "        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n",
    "        :return: attention weighted encoding, weights\n",
    "        \"\"\"\n",
    "        att1 = self.encoder_att(encoder_out)     # (batch_size, num_pixels, attention_dim)\n",
    "        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n",
    "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n",
    "        alpha = self.softmax(att)  # (batch_size, num_pixels)\n",
    "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n",
    "\n",
    "        return attention_weighted_encoding, alpha\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderWithAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n",
    "        \"\"\"\n",
    "        :param attention_dim: size of attention network\n",
    "        :param embed_dim: embedding size\n",
    "        :param decoder_dim: size of decoder's RNN\n",
    "        :param vocab_size: size of vocabulary\n",
    "        :param encoder_dim: feature size of encoded images\n",
    "        :param dropout: dropout\n",
    "        \"\"\"\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n",
    "        self.dropout = nn.Dropout(p=self.dropout)\n",
    "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n",
    "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n",
    "        self.init_weights()  # initialize some layers with the uniform distribution\n",
    "\n",
    "        \n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initializes some parameters with values from the uniform distribution, for easier convergence.\n",
    "        \"\"\"\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "        \n",
    "    def load_pretrained_embeddings(self, weight_matrix):\n",
    "        \"\"\"\n",
    "        Loads embedding layer with pre-trained embeddings.\n",
    "        :param embeddings: pre-trained embeddings\n",
    "        \"\"\"\n",
    "        self.embedding.weight = nn.Parameter(weight_matrix)\n",
    "\n",
    "        \n",
    "    def fine_tune_embeddings(self, fine_tune=False):\n",
    "        \"\"\"\n",
    "        Allow fine-tuning of embedding layer? (Only makes sense to not-allow if using pre-trained embeddings).\n",
    "        :param fine_tune: Allow?\n",
    "        \"\"\"\n",
    "        for p in self.embedding.parameters():\n",
    "            p.requires_grad = fine_tune\n",
    "\n",
    "            \n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        \"\"\"\n",
    "        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n",
    "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
    "        :return: hidden state, cell state\n",
    "        \"\"\"\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)\n",
    "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
    "        c = self.init_c(mean_encoder_out)\n",
    "        return h, c\n",
    "\n",
    "    \n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n",
    "        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n",
    "        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n",
    "        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = encoder_out.size(0)\n",
    "        encoder_dim = encoder_out.size(-1)\n",
    "        vocab_size = self.vocab_size\n",
    "\n",
    "        # Flatten image\n",
    "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "\n",
    "        # Sort input data by decreasing lengths; why? apparent below\n",
    "        caption_lengths, sort_ind = caption_lengths.sort(descending=True)\n",
    "        encoder_out = encoder_out[sort_ind]\n",
    "        encoded_captions = encoded_captions[sort_ind]\n",
    "\n",
    "        # Embedding\n",
    "        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n",
    "\n",
    "        # Initialize LSTM state\n",
    "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
    "\n",
    "        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n",
    "        # So, decoding lengths are actual lengths - 1\n",
    "        decode_lengths = (caption_lengths - 1).tolist()\n",
    "\n",
    "        # Create tensors to hold word predicion scores and alphas\n",
    "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n",
    "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n",
    "\n",
    "        # At each time-step, decode by\n",
    "        # attention-weighing the encoder's output based on the decoder's previous hidden state output\n",
    "        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n",
    "        for t in range(max(decode_lengths)):\n",
    "            batch_size_t = sum([l > t for l in decode_lengths])\n",
    "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
    "                                                                h[:batch_size_t])\n",
    "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            h, c = self.decode_step(\n",
    "                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
    "                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n",
    "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "            alphas[:batch_size_t, t, :] = alpha\n",
    "\n",
    "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Process\n",
    "The following classes describe the training process. The main-class for training is the **_Trainer()_** class. The _Trainer()_ class is called by parsing a dataloader-object (=instance of class _LoadData()_ ), the data-object and the arguments instance. Within the class, _Trainer()_ comprises the following structure: <br>\n",
    "<br>\n",
    "Structure of **_Trainer()-class_**:\n",
    "- **Main-method**: coordinates the training and loads the models (encoder and decoder). Moreover, it calls the method save_checkpoint() to save checkpoints\n",
    "- **TrainModel**: executes the training for every epoch. It iterates over all steps (as defined in Arguments) and is responsible for managing forward- and backward propagation. For every step in n-steps, it calls a new batch from the dataloader-instance and passes it to the model.\n",
    "- **EvalModel**: evaluate the training process (set the test-steps in arguments!). For every step, it load a batch from the test-set (=val2014) with a specific batch-size (current = 1). It then executes the forwardpass without calculating the gradients and without backpropagation. To calculate the BLEU-score for every datapoint, we retrieve its reference captions, and pass them together with the predicted caption to the NLTK toolkit _corpus-bleu()_ . \n",
    "- **Utils-methods**: besides the training and evaluation, the _Trainer()_ class also implements various helper-methods for adjusting the learning rate, saving the checkpoints, clipping gradients or calculating the accuracies.\n",
    "<br> \n",
    "<br>\n",
    "Furthermore, we save all calculated metrics in an instance of the class AverageMeter(), which is instantiated for saving losses, and accuracies during training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Keeps track of most recent, average, sum, and count of a metric.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchtext.vocab as vocab\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "    \n",
    "    '''\n",
    "    --------------------------------------------------------------------------------------------------\n",
    "    Utils methods\n",
    "    --------------------------------------------------------------------------------------------------\n",
    "    '''\n",
    "    def Adjust_LR(optimizer, shrink_factor):\n",
    "        \n",
    "        print(\"\\nDECAYING learning rate.\")\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * shrink_factor\n",
    "        print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))\n",
    "    \n",
    "        \n",
    "    # Function to clip gradients\n",
    "    def clip_gradient(optimizer, grad_clip):\n",
    "   \n",
    "        for group in optimizer.param_groups:\n",
    "            for param in group['params']:\n",
    "                if param.grad is not None:\n",
    "                    param.grad.data.clamp_(-grad_clip, grad_clip)\n",
    "     \n",
    "    \n",
    "    def accuracy(scores, targets, k):\n",
    "        \n",
    "        batch_size = targets.size(0)\n",
    "        _, ind = scores.topk(k, 1, True, True)\n",
    "        correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n",
    "        correct_total = correct.view(-1).float().sum()  # 0D tensor\n",
    "        \n",
    "        return correct_total.item() * (100.0 / batch_size)\n",
    "    \n",
    "    \n",
    "    def save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "                    bleu4, is_best):\n",
    "        \n",
    "        # State to be saved\n",
    "        state = {'epoch': epoch,\n",
    "                 'epochs_since_improvement': epochs_since_improvement,\n",
    "                 'bleu-4': bleu4,\n",
    "                 'encoder': encoder,\n",
    "                 'decoder': decoder,\n",
    "                 'encoder_optimizer': encoder_optimizer,\n",
    "                 'decoder_optimizer': decoder_optimizer}\n",
    "        \n",
    "        filename = 'checkpoint_' + data_name + 'glove300d.pth.tar'\n",
    "        torch.save(state, filename)\n",
    "        \n",
    "        # If this checkpoint is the best so far, store a copy so it doesn't get overwritten by a worse checkpoint\n",
    "        if is_best:\n",
    "            torch.save(state, 'BEST_' + filename)\n",
    "            \n",
    "            \n",
    "    def Load_Glove(data_object):\n",
    "        \n",
    "        print(\"Load Wordembeddings\")\n",
    "        # Load Glove using Torchtext\n",
    "        glove = vocab.GloVe(name='6B', dim=300)\n",
    "        \n",
    "        matrix_len = len(data_object.vocab._token_to_idx)\n",
    "        weights_matrix = np.zeros((matrix_len, 300))\n",
    "        words_found = 0\n",
    "\n",
    "        for i, word in enumerate(data_object.vocab._token_to_idx):\n",
    "            try: \n",
    "                weights_matrix[i] = glove[word]\n",
    "                words_found += 1\n",
    "            except KeyError:\n",
    "                weights_matrix[i] = np.random.normal(scale=0.6, size=(300, ))\n",
    "    \n",
    "        return weights_matrix\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    --------------------------------------------------------------------------------------------------\n",
    "    Training methods\n",
    "    --------------------------------------------------------------------------------------------------\n",
    "    '''\n",
    "    \n",
    "    #Function to execute the forward and backward pass\n",
    "    def TrainModel(encoder, decoder, criterion, encoder_optimizer,\n",
    "                   decoder_optimizer, epoch, losses, top5accs, args, dataloader):         \n",
    "            \n",
    "            \n",
    "        # Iterate of over the steps and load a new dataset for every step\n",
    "        for i_step in range(1, args.total_step+1):\n",
    "                \n",
    "            # Set models to train-mode\n",
    "            decoder.train()  \n",
    "            encoder.train()\n",
    "                \n",
    "            # Get dataset for each step\n",
    "            batch = dataloader.CreateDataloader(mode = 'train')\n",
    "            img, cap, cap_len = next(iter(batch))\n",
    "                \n",
    "            # Move data to GPU\n",
    "            img = img.to(device)\n",
    "            cap = cap.to(device)\n",
    "            cap_len = cap_len.to(device)\n",
    "                \n",
    "            # Forward propagation\n",
    "            img = encoder(img)\n",
    "            scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(img, cap, cap_len)\n",
    "                \n",
    "            # Targets start after [BOS] and end before [EOS]\n",
    "            targets = caps_sorted[:, 1:]\n",
    "                \n",
    "            # Remove timesteps that we didn't decode at, or are pads\n",
    "            # pack_padded_sequence is an easy trick to do this\n",
    "        \n",
    "            scores = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n",
    "            scores = scores[0]\n",
    "            targets = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
    "            targets = targets[0]\n",
    "                \n",
    "            # Calculate loss\n",
    "            loss = criterion(scores, targets)\n",
    "\n",
    "            # Add doubly stochastic attention regularization\n",
    "            loss += args.alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
    "\n",
    "            # Back prop.\n",
    "            decoder_optimizer.zero_grad()\n",
    "                \n",
    "            if encoder_optimizer is not None:\n",
    "                encoder_optimizer.zero_grad()\n",
    "                \n",
    "            loss.backward()\n",
    "                \n",
    "            # Clip gradients by calling function clip_gradient()\n",
    "            if args.grad_clip is not None:\n",
    "                Trainer.clip_gradient(decoder_optimizer, args.grad_clip)\n",
    "                    \n",
    "                if encoder_optimizer is not None:\n",
    "                    Trainer.clip_gradient(encoder_optimizer, args.grad_clip)\n",
    "\n",
    "            # Update weights\n",
    "            decoder_optimizer.step()\n",
    "                \n",
    "            if encoder_optimizer is not None:\n",
    "                encoder_optimizer.step()\n",
    "\n",
    "            # Keep track of metrics\n",
    "            top5 = Trainer.accuracy(scores, targets, 5)\n",
    "            losses.update(loss.item(), sum(decode_lengths))\n",
    "            top5accs.update(top5, sum(decode_lengths))\n",
    "            \n",
    "            \n",
    "        return encoder, decoder, losses, top5accs \n",
    "            \n",
    "       \n",
    "        \n",
    "    #Function to evaluate the model during training on the validation set   \n",
    "    def EvalModel(encoder, decoder, criterion, encoder_optimizer,\n",
    "                   decoder_optimizer, epoch, losses, top5accs, args, dataloader):\n",
    "        \n",
    "        \n",
    "        # Add reference list for calculating Blue-Scores\n",
    "        true_captions = list() \n",
    "        predictions = list() \n",
    "        \n",
    "        # Iterate of over the steps and load a new dataset for every step\n",
    "        for i_step in range(1, args.test_total_step+1):\n",
    "            \n",
    "            decoder.eval()\n",
    "            encoder.eval()\n",
    "        \n",
    "            # Disable gradient calculation for evaluation\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                # Get dataset for each test step\n",
    "                batch = dataloader.CreateDataloader(mode = 'test')\n",
    "                img, cap, cap_len, cap_reference = next(iter(batch))\n",
    "\n",
    "                # Move data to GPU\n",
    "                img = img.to(device)\n",
    "                cap = cap.to(device)\n",
    "                cap_len = cap_len.to(device)\n",
    "\n",
    "                # Forward propagation\n",
    "                img = encoder(img)\n",
    "                scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(img, cap, cap_len)\n",
    "\n",
    "                # Targets start after [BOS] and end before [EOS]\n",
    "                targets = caps_sorted[:, 1:]\n",
    "\n",
    "                # Remove timesteps that we didn't decode at, or are pads\n",
    "                # pack_padded_sequence is an easy trick to do this\n",
    "                \n",
    "                scores_copy = scores.clone()\n",
    "\n",
    "                scores = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n",
    "                scores = scores[0]\n",
    "                targets = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
    "                targets = targets[0]\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(scores, targets)\n",
    "\n",
    "                # Add doubly stochastic attention regularization\n",
    "                loss += args.alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
    "                \n",
    "                # Keep track of metrics\n",
    "                top5 = Trainer.accuracy(scores, targets, 5)\n",
    "                losses.update(loss.item(), sum(decode_lengths))\n",
    "                top5accs.update(top5, sum(decode_lengths))\n",
    "            \n",
    "                \n",
    "                # Store references for calculating BLEU scores\n",
    "                for index in sort_ind:\n",
    "                    \n",
    "                    img_captions = []\n",
    "                    for caption in cap_reference[index]:\n",
    "                        img_caption = dataloader.test_cap_tokens[caption] # Retrieve the caption based on its index\n",
    "                        img_caption_len = dataloader.test_caps_len[caption] # Retrieve the length of the caption\n",
    "                        img_caption = img_caption[1:(img_caption_len)] # Remove start-token and paddings\n",
    "                        img_captions.append(img_caption) # Add all tokenized captions to the array\n",
    "                        \n",
    "                    true_captions.append(img_captions)\n",
    "                \n",
    "                # Get predictions\n",
    "                _, preds = torch.max(scores_copy, dim=2)\n",
    "                preds = preds.tolist() # Save tensor as list\n",
    "                temp_preds = []\n",
    "                \n",
    "                for i, prediction in enumerate(preds):\n",
    "                    temp_preds.append(preds[i][:decode_lengths[i]]) # if any, remove paddings\n",
    "                \n",
    "                preds = temp_preds\n",
    "                predictions.extend(preds)\n",
    "                \n",
    "        # Calculate BLEU-Scores using NLTK toolkit\n",
    "        bleu = corpus_bleu(true_captions, predictions)\n",
    "        \n",
    "        return bleu, losses, top5accs\n",
    "            \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Main-function to be called, which loads the optimizer, then executes the training and evaluation\n",
    "    \"\"\"\n",
    "    def Main(data_object, dataloader, args):\n",
    "        \n",
    "        print(\"Initialize Training by setting-up models\")\n",
    "        # Initialize a new model or load existing checkpoints\n",
    "        if args.checkpoint is None:\n",
    "            decoder = DecoderWithAttention(attention_dim = args.attention_dim,\n",
    "                                       embed_dim = args.emb_dim,\n",
    "                                       decoder_dim = args.decoder_dim,\n",
    "                                       vocab_size = len(data_object.vocab._token_to_idx),\n",
    "                                       dropout= args.dropout)\n",
    "            decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, decoder.parameters()),\n",
    "                                             lr= args.decoder_lr)\n",
    "            encoder = Encoder()\n",
    "            encoder.fine_tune(args.fine_tune_encoder)\n",
    "            encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
    "                                                 lr=args.encoder_lr) if args.fine_tune_encoder else None\n",
    "\n",
    "        \n",
    "        else:\n",
    "            print(\"Load existing checkpoint\")\n",
    "            checkpoint = torch.load(args.checkpoint)\n",
    "            start_epoch = checkpoint['epoch'] + 1\n",
    "            \n",
    "            epochs_since_improvement = checkpoint['epochs_since_improvement']\n",
    "            best_bleu4 = checkpoint['bleu-4']\n",
    "            \n",
    "            decoder = checkpoint['decoder']\n",
    "            decoder_optimizer = checkpoint['decoder_optimizer']\n",
    "            \n",
    "            encoder = checkpoint['encoder']\n",
    "            encoder_optimizer = checkpoint['encoder_optimizer']\n",
    "            \n",
    "            if args.fine_tune_encoder is True and encoder_optimizer is None:\n",
    "                \n",
    "                encoder.fine_tune(args.fine_tune_encoder)\n",
    "                encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
    "                                                 lr=args.encoder_lr)\n",
    "                \n",
    "        # Load Glove weight matrix for embedding layer and update weigths\n",
    "        weigth_matrix = Trainer.Load_Glove(data_object)\n",
    "        weight_matrix = torch.Tensor(weigth_matrix)\n",
    "        decoder.load_pretrained_embeddings(weight_matrix)\n",
    "        print(\"Word embeddings loaded and embedding weights updated\")\n",
    "        \n",
    "        # Move the models to the GPU\n",
    "        encoder = encoder.to(device)\n",
    "        decoder = decoder.to(device)\n",
    "        \n",
    "        # Loss function\n",
    "        criterion = nn.CrossEntropyLoss().to(device)\n",
    "        \n",
    "        # Call class AverageMeter() to instantiate metrics\n",
    "        losses = AverageMeter()     # loss (per word decoded)\n",
    "        top5accs = AverageMeter()   # top5 accuracy\n",
    "        losses_val = AverageMeter()\n",
    "        top5accs_val = AverageMeter()\n",
    "        \n",
    "        \n",
    "        # Start training\n",
    "        for epoch in tqdm(range(args.start_epoch, args.epochs)):\n",
    "            \n",
    "            # Decay learning rate if there is no improvement for 8 consecutive epochs, and terminate training after 20\n",
    "            if args.epochs_since_improvement == 20:\n",
    "                break\n",
    "\n",
    "            if args.epochs_since_improvement > 0 and args.epochs_since_improvement % 8 == 0:\n",
    "\n",
    "                Trainer.Adjust_LR(decoder_optimizer, 0.8)\n",
    "\n",
    "                if args.fine_tune_encoder:\n",
    "                    Trainer.Adjust_LR(encoder_optimizer, 0.8)\n",
    "                    \n",
    "                    \n",
    "            # Now, start training\n",
    "            encoder, decoder, losses, top5accs = Trainer.TrainModel(encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, \n",
    "                                                                    epoch, losses, top5accs, args, dataloader)\n",
    "             \n",
    "            print(\"-----------Metrics for Epoch %s-----------\" %epoch)\n",
    "            print(\"Average training loss: %s\" %losses.avg)\n",
    "            print(\"Average training accuracy: %s\" %top5accs.avg)\n",
    "            \n",
    "            # After training for n-steps in each epoch, evaluate the output\n",
    "            current_bleu, losses_val, top5accs_val = Trainer.EvalModel(encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, \n",
    "                             epoch, losses_val, top5accs_val, args, dataloader)\n",
    "            \n",
    "            print(\"Average validation loss: %s\" %losses_val.avg)\n",
    "            print(\"Average validation accuracy: %s\" %top5accs_val.avg)\n",
    "            print(\"BLEU-score: %s\" %current_bleu)\n",
    "            \n",
    "            \n",
    "            # Compare BLEU Score with previous scores\n",
    "            is_best_bleu = current_bleu > args.best_bleu\n",
    "            args.best_bleu = max(current_bleu, args.best_bleu)\n",
    "            \n",
    "            if not is_best_bleu:\n",
    "                args.epochs_since_improvement += 1\n",
    "                print(\"Epochs since improvement: %s\" %args.epochs_since_improvement)\n",
    "                \n",
    "            else:\n",
    "                args.epochs_since_improvement = 0\n",
    "            \n",
    "            # Save checkpoint\n",
    "            Trainer.save_checkpoint(args.model_name, epoch, args.epochs_since_improvement, encoder, decoder,\n",
    "                                    encoder_optimizer, decoder_optimizer, current_bleu, is_best_bleu)\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Instantiate Training\n",
    "Finally, to start training we need instantiate an object of the class **_DataPreprocess_**, which we'll call data_object. Using the data_object, we instantiate an object of the class **_LoadData_**, which includes the dataloader. Lastly, we use both the data_object as well as the dataloader-object to start the training by calling the Main()-method of the **_Trainer()-class_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=12.06s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.87s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=6.58s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.43s)\n",
      "creating index...\n",
      "index created!\n",
      "Load existing vocabulary\n",
      "Start Tokenization\n",
      "Start Tokenization\n",
      "Start getting indices...\n",
      "Start getting indices...\n"
     ]
    }
   ],
   "source": [
    "data_object = DataPreprocess.ProcessData(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = LoadData(data_object, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize Training by setting-up models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" to /root/.cache/torch/checkpoints/resnet101-5d3b4d8f.pth\n",
      "73.8%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Wordembeddings\n",
      "Word embeddings loaded and embedding weights updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Metrics for Epoch 0-----------\n",
      "Average training loss: 5.4620256498198705\n",
      "Average training accuracy: 48.058848133514466\n",
      "Average validation loss: 4.579207232129352\n",
      "Average validation accuracy: 58.711629917855596\n",
      "BLEU-score: 0.10537740475960217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DecoderWithAttention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Attention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "  1%|          | 1/100 [19:18<31:52:01, 1158.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Metrics for Epoch 1-----------\n",
      "Average training loss: 5.0299822298420835\n",
      "Average training accuracy: 53.05985571763859\n",
      "Average validation loss: 4.414589353679201\n",
      "Average validation accuracy: 60.27157249233465\n",
      "BLEU-score: 0.13026540328835762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [38:35<31:31:39, 1158.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Metrics for Epoch 2-----------\n",
      "Average training loss: 4.801371099721451\n",
      "Average training accuracy: 55.7058780772306\n",
      "Average validation loss: 4.29830440969652\n",
      "Average validation accuracy: 61.463343535927706\n",
      "BLEU-score: 0.14739830734888754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [57:36<31:03:57, 1152.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Metrics for Epoch 3-----------\n",
      "Average training loss: 4.65055874203267\n",
      "Average training accuracy: 57.45861516265166\n",
      "Average validation loss: 4.190259289562787\n",
      "Average validation accuracy: 63.13526623305038\n",
      "BLEU-score: 0.17648947242967197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [1:16:38<30:39:22, 1149.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Metrics for Epoch 4-----------\n",
      "Average training loss: 4.5383444568536095\n",
      "Average training accuracy: 58.77304597287373\n",
      "Average validation loss: 4.121639466857221\n",
      "Average validation accuracy: 63.94641755530096\n",
      "BLEU-score: 0.15736580681628304\n",
      "Epochs since improvement: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [1:35:33<30:13:23, 1145.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Metrics for Epoch 5-----------\n",
      "Average training loss: 4.451857416399863\n",
      "Average training accuracy: 59.786563287074195\n",
      "Average validation loss: 4.07681141800937\n",
      "Average validation accuracy: 64.56531300395837\n",
      "BLEU-score: 0.16276560617311414\n",
      "Epochs since improvement: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [1:54:26<29:48:45, 1141.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Metrics for Epoch 6-----------\n",
      "Average training loss: 4.379413872914554\n",
      "Average training accuracy: 60.646020223009664\n",
      "Average validation loss: 4.033044104995606\n",
      "Average validation accuracy: 65.07867132867133\n",
      "BLEU-score: 0.18070952051567885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [2:13:57<29:43:02, 1150.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Metrics for Epoch 7-----------\n",
      "Average training loss: 4.319004536728151\n",
      "Average training accuracy: 61.358949963240065\n",
      "Average validation loss: 4.008871428314521\n",
      "Average validation accuracy: 65.32244897959184\n",
      "BLEU-score: 0.176063208326808\n",
      "Epochs since improvement: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [2:34:14<29:54:40, 1170.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Metrics for Epoch 8-----------\n",
      "Average training loss: 4.266622715069842\n",
      "Average training accuracy: 61.97568304356892\n",
      "Average validation loss: 3.983129996413365\n",
      "Average validation accuracy: 65.62666279688483\n",
      "BLEU-score: 0.16494754940612683\n",
      "Epochs since improvement: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [2:54:03<29:43:25, 1175.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Metrics for Epoch 9-----------\n",
      "Average training loss: 4.220905399037353\n",
      "Average training accuracy: 62.5139767319949\n",
      "Average validation loss: 3.9637099953373496\n",
      "Average validation accuracy: 65.8991991643454\n",
      "BLEU-score: 0.16127689819954236\n",
      "Epochs since improvement: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [3:13:49<29:28:42, 1179.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Metrics for Epoch 10-----------\n",
      "Average training loss: 4.180401458799817\n",
      "Average training accuracy: 62.997093947986684\n",
      "Average validation loss: 3.9379384471622156\n",
      "Average validation accuracy: 66.1772272133613\n",
      "BLEU-score: 0.1701431131428906\n",
      "Epochs since improvement: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [3:33:29<29:09:12, 1179.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Metrics for Epoch 11-----------\n",
      "Average training loss: 4.144112720953953\n",
      "Average training accuracy: 63.43124118974898\n",
      "Average validation loss: 3.927592958484417\n",
      "Average validation accuracy: 66.22062663185379\n",
      "BLEU-score: 0.16053332693429215\n",
      "Epochs since improvement: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [3:53:12<28:51:18, 1180.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Metrics for Epoch 12-----------\n",
      "Average training loss: 4.111190110739318\n",
      "Average training accuracy: 63.820647182842464\n",
      "Average validation loss: 3.898494093590803\n",
      "Average validation accuracy: 66.63431287234755\n",
      "BLEU-score: 0.18175892224047738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [4:14:57<29:25:41, 1217.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Metrics for Epoch 13-----------\n",
      "Average training loss: 4.081435444684267\n",
      "Average training accuracy: 64.17260490405505\n",
      "Average validation loss: 3.875614231415796\n",
      "Average validation accuracy: 66.94965326367509\n",
      "BLEU-score: 0.1828670718636703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [4:36:44<29:43:49, 1244.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Metrics for Epoch 14-----------\n",
      "Average training loss: 4.053825528213686\n",
      "Average training accuracy: 64.50765038597056\n",
      "Average validation loss: 3.8589352743475325\n",
      "Average validation accuracy: 67.14517719389826\n",
      "BLEU-score: 0.17614536809537082\n",
      "Epochs since improvement: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [4:58:31<29:49:30, 1263.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Metrics for Epoch 15-----------\n",
      "Average training loss: 4.028273591474722\n",
      "Average training accuracy: 64.81202701845017\n",
      "Average validation loss: 3.8414974854278667\n",
      "Average validation accuracy: 67.34655078761543\n",
      "BLEU-score: 0.1947429163765393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [5:20:10<29:43:48, 1274.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Metrics for Epoch 16-----------\n",
      "Average training loss: 4.004671805695419\n",
      "Average training accuracy: 65.0937883144438\n",
      "Average validation loss: 3.8284119077053114\n",
      "Average validation accuracy: 67.47483521536968\n",
      "BLEU-score: 0.1979634245773051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [5:42:01<29:37:55, 1285.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Metrics for Epoch 17-----------\n",
      "Average training loss: 3.9828530739245287\n",
      "Average training accuracy: 65.35191556067589\n",
      "Average validation loss: 3.8091073006917804\n",
      "Average validation accuracy: 67.65897906011773\n",
      "BLEU-score: 0.2058748712565859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [6:03:56<29:28:24, 1293.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Metrics for Epoch 18-----------\n",
      "Average training loss: 3.962007783906738\n",
      "Average training accuracy: 65.60099881266717\n",
      "Average validation loss: 3.7964951560225355\n",
      "Average validation accuracy: 67.80296106744653\n",
      "BLEU-score: 0.19787497513871777\n",
      "Epochs since improvement: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [6:25:45<29:12:50, 1298.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Metrics for Epoch 19-----------\n",
      "Average training loss: 3.9422114229894625\n",
      "Average training accuracy: 65.841692452517\n",
      "Average validation loss: 3.7799538403686053\n",
      "Average validation accuracy: 68.01641408658648\n",
      "BLEU-score: 0.1803975204055632\n",
      "Epochs since improvement: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [6:47:29<28:53:36, 1300.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Metrics for Epoch 20-----------\n",
      "Average training loss: 3.9238864055350966\n",
      "Average training accuracy: 66.06056207652797\n",
      "Average validation loss: 3.765887196562213\n",
      "Average validation accuracy: 68.15643613500993\n",
      "BLEU-score: 0.19478964416259492\n",
      "Epochs since improvement: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [7:09:19<28:35:57, 1303.26s/it]"
     ]
    }
   ],
   "source": [
    "train = Trainer.Main(data_object, dataloader, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "To evaluate the performance of the image captioning model, the BLEU-4 score is not sufficient. Therefore, we will manually select images from the dataset and compare the generated captions. For this, run the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import csv\n",
    "\n",
    "\n",
    "class TestModel(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "       \n",
    "        # Test parameters\n",
    "        self.transform = transforms.Compose([ \n",
    "                    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "                    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "                    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "                    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "                    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                                     (0.229, 0.224, 0.225))])\n",
    "\n",
    "        self.test_mode = ''\n",
    "        self.beam_size = ''\n",
    "        self.args = ''\n",
    "        self.vocab = ''\n",
    "        self.vocab_size = ''\n",
    "        self.bleu = list()\n",
    "        \n",
    "        # Image for inference\n",
    "        self.image = ''\n",
    "        self.image_name = ''\n",
    "        \n",
    "        # Thumbnails\n",
    "        self.thumbnails = []\n",
    "        \n",
    "        # Dataloader for inference\n",
    "        self.dataloader = ''\n",
    "        \n",
    "    @classmethod\n",
    "    def MainTest(cls, test_mode, model_path, beam_size, args, dataloader = None):\n",
    "        \n",
    "        # Load parameters\n",
    "        cls.test_mode =test_mode\n",
    "        cls.beam_size = beam_size\n",
    "        cls.args = args\n",
    "        \n",
    "        TestModel.LoadVocab(cls)\n",
    "        \n",
    "        # Load the encoder and decoder networks\n",
    "        encoder, decoder = TestModel.LoadModel(model_path)\n",
    "        \n",
    "        # If in test_mode, get test-set and evaluate based on it\n",
    "        if test_mode == 'test':\n",
    "            cls.dataloader = dataloader\n",
    "            \n",
    "        # If in inference_mode, get\n",
    "        elif test_mode == 'inference_manually':\n",
    "            \n",
    "            image_path = str(input(\"Enter path to image here: \"))\n",
    "            cls.image = TestModel.TransformImage(cls, image_path)\n",
    "            cls.image_name = image_path\n",
    "            \n",
    "            TestModel.Evaluate(cls, encoder, decoder)\n",
    "            \n",
    "        elif test_mode == 'inference_thumbnails':\n",
    "            \n",
    "            # Load thumbnails \n",
    "            path = args.dir_path + '/thumbnails.pkl'\n",
    "            file = open(path, 'rb')\n",
    "            thumbnails = pickle.load(file)\n",
    "            file.close()\n",
    "            \n",
    "            # Save thumbnails into class variable\n",
    "            cls.thumbnails = thumbnails\n",
    "            \n",
    "        \n",
    "        # Run pre-trained model\n",
    "        captions = TestModel.Evaluate(cls, encoder, decoder)\n",
    "        \n",
    "        if test_mode == 'inference_thumbnails':\n",
    "            \n",
    "            path = args.dir_path + '/captions.csv'\n",
    "            \n",
    "            # Save captions\n",
    "            with open(path, 'w', newline='') as myfile:\n",
    "\n",
    "                fieldnames = ['caption', 'thumbnail_id']\n",
    "                wr = csv.DictWriter(myfile, quoting=csv.QUOTE_ALL, fieldnames=fieldnames)\n",
    "                wr.writeheader()\n",
    "\n",
    "                for i, row in enumerate(captions):\n",
    "                    separator = ' '\n",
    "                    caption = separator.join(row)\n",
    "                    wr.writerow({'caption': caption, 'thumbnail_id': cls.thumbnails[i]})\n",
    "    \n",
    "            \n",
    "        return cls()\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def LoadVocab(self):\n",
    "        \n",
    "        file = open(args.vocab_path, 'rb')\n",
    "        self.vocab = pickle.load(file)\n",
    "        self.vocab_size = len(self.vocab._token_to_idx)\n",
    "        \n",
    "        print(self.vocab_size)\n",
    "        \n",
    "        file.close()\n",
    "            \n",
    "            \n",
    "    @staticmethod\n",
    "    def LoadModel(model_path):\n",
    "        \n",
    "        # Load checkpoint from model_path\n",
    "        checkpoint = torch.load(model_path, map_location = torch.device('cpu')) \n",
    "        encoder = checkpoint['encoder']\n",
    "        decoder = checkpoint['decoder']\n",
    "        \n",
    "        # send encoder and decoder to device\n",
    "        encoder.to(device)\n",
    "        decoder.to(device)\n",
    "        \n",
    "        # set both networks to eval\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        \n",
    "        return encoder, decoder\n",
    "    \n",
    " \n",
    "    @staticmethod\n",
    "    def TransformImage(self, image_path):\n",
    "        \n",
    "        # Convert image to tensor and pre-process using transform\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        transform = transforms.Compose([ \n",
    "                    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "                    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "                    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "                    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "                    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                                     (0.229, 0.224, 0.225))])\n",
    "        image = transform(image).unsqueeze_(0)\n",
    "        image.to(device)\n",
    "        \n",
    "        \n",
    "        return image\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def Evaluate(self, encoder, decoder):\n",
    "        \n",
    "        # Depending on the test mode, either evaluate on the test-dataset or on a single image\n",
    "        if self.test_mode == 'test':\n",
    "            step_size = self.args.test_total_step\n",
    "            \n",
    "            # Add reference list for calculating Blue-Scores\n",
    "            true_captions = list() \n",
    "            predictions = list()\n",
    "            \n",
    "        # We don't need any reference point for our image\n",
    "        elif self.test_mode == 'inference_manually':\n",
    "            step_size = 1\n",
    "            predictions = list()\n",
    "            \n",
    "        elif self.test_mode == 'inference_thumbnails':\n",
    "            step_size = len(self.thumbnails)\n",
    "            predictions = list()\n",
    "            \n",
    "            \n",
    "        # Iterate of over the steps and load a new dataset for every step\n",
    "        for i_step in tqdm(range(1, step_size+1)):\n",
    "        \n",
    "            # Disable gradient calculation for evaluation\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                # Load beam_size\n",
    "                k = self.beam_size\n",
    "                \n",
    "                # If test, get batch, else get image\n",
    "                if self.test_mode == 'test':\n",
    "                    # Get dataset for each test step\n",
    "                    batch = self.dataloader.CreateDataloader(mode = 'test')\n",
    "                    img, cap, cap_len, cap_reference = next(iter(batch))\n",
    "\n",
    "                    # Move data to GPU\n",
    "                    img = img.to(device)\n",
    "                    cap = cap.to(device)\n",
    "                    cap_len = cap_len.to(device)\n",
    "                    \n",
    "                elif self.test_mode == 'inference_manually':\n",
    "                    img_name = self.image_name\n",
    "                    img = self.image\n",
    "                    img = img.to(device)\n",
    "                    \n",
    "                \n",
    "                elif self.test_mode == 'inference_thumbnails':\n",
    "                    img_name = self.thumbnails[i_step-1]\n",
    "                    img_path = self.args.thumbnails + img_name\n",
    "                    img = TestModel.TransformImage(self, img_path)\n",
    "                    \n",
    "                    img = img.to(device)\n",
    "                    \n",
    "                try:\n",
    "                    # Forward propagation\n",
    "                    encoder_out = encoder(img)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
    "                    enc_image_size = encoder_out.size(1)\n",
    "                    encoder_dim = encoder_out.size(3)\n",
    "\n",
    "                    # Flatten encoding\n",
    "                    encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n",
    "                    num_pixels = encoder_out.size(1)\n",
    "\n",
    "                    # We'll treat the problem as having a batch size of k\n",
    "                    encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n",
    "\n",
    "                    # Tensor to store top k previous words at each step; now they're just <start>\n",
    "                    k_prev_words = torch.LongTensor([[self.vocab.lookup_token('[BOS]')]] * k).to(device)  # (k, 1)\n",
    "\n",
    "                    # Tensor to store top k sequences; now they're just <start>\n",
    "                    seqs = k_prev_words  # (k, 1)\n",
    "\n",
    "                    # Tensor to store top k sequences' scores; now they're just 0\n",
    "                    top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
    "\n",
    "                    # Lists to store completed sequences and scores\n",
    "                    complete_seqs = list()\n",
    "                    complete_seqs_scores = list()\n",
    "\n",
    "                    # Start decoding\n",
    "                    step = 1\n",
    "                    h, c = decoder.init_hidden_state(encoder_out)\n",
    "\n",
    "                    # s is a number less than or equal to k, because sequences are removed from this process once they hit [EOS]\n",
    "                    while True:\n",
    "\n",
    "                        embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n",
    "\n",
    "                        awe, _ = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n",
    "\n",
    "                        gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n",
    "                        awe = gate * awe\n",
    "\n",
    "                        h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n",
    "\n",
    "                        scores = decoder.fc(h)  # (s, vocab_size)\n",
    "                        scores = F.log_softmax(scores, dim=1)\n",
    "\n",
    "                        # Add\n",
    "                        scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n",
    "\n",
    "                        # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
    "                        if step == 1:\n",
    "                            top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n",
    "\n",
    "                        else:\n",
    "                            # Unroll and find top scores, and their unrolled indices\n",
    "                            top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n",
    "\n",
    "                        # Convert unrolled indices to actual indices of scores\n",
    "                        prev_word_inds = top_k_words / self.vocab_size  # (s)\n",
    "                        next_word_inds = top_k_words % self.vocab_size  # (s)\n",
    "\n",
    "                        # Add new words to sequences\n",
    "                        seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
    "\n",
    "                        # Which sequences are incomplete (didn't reach <end>)?\n",
    "                        incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
    "                                           next_word != self.vocab.lookup_token('[EOS]')]\n",
    "\n",
    "                        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
    "\n",
    "                        # Set aside complete sequences\n",
    "                        if len(complete_inds) > 0:\n",
    "                            complete_seqs.extend(seqs[complete_inds].tolist())\n",
    "                            complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
    "\n",
    "                        k -= len(complete_inds)  # reduce beam length accordingly\n",
    "\n",
    "                        # Proceed with incomplete sequences\n",
    "                        if k == 0:\n",
    "                            break\n",
    "\n",
    "                        seqs = seqs[incomplete_inds]\n",
    "                        h = h[prev_word_inds[incomplete_inds]]\n",
    "                        c = c[prev_word_inds[incomplete_inds]]\n",
    "                        encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
    "                        top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
    "                        k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
    "\n",
    "                        # Break if things have been going on too long\n",
    "                        if step > 50:\n",
    "                            break\n",
    "\n",
    "                        step += 1\n",
    "                    \n",
    "                \n",
    "                    i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
    "                    seq = complete_seqs[i]\n",
    "                \n",
    "                    if self.test_mode == 'test':\n",
    "                        img_captions = []\n",
    "                        for caption in cap_reference:\n",
    "                            img_caption = dataloader.test_cap_tokens[caption] # Retrieve the caption based on its index\n",
    "                            img_caption_len = dataloader.test_caps_len[caption] # Retrieve the length of the caption\n",
    "                            img_caption = img_caption[1:(img_caption_len)] # Remove start-token and paddings\n",
    "                            img_captions.append(img_caption) # Add all tokenized captions to the array\n",
    "\n",
    "                        true_captions.append(img_captions)\n",
    "\n",
    "                        # Get predictions\n",
    "                        temp_preds = []\n",
    "\n",
    "                        for i, prediction in enumerate(seq):\n",
    "                            temp_preds.append(preds[i][:decode_lengths[i]]) # if any, remove paddings\n",
    "\n",
    "                        seq = temp_preds\n",
    "                        predictions.extend(seq)\n",
    "\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        predicted_caption = list()\n",
    "\n",
    "                        for token in seq:\n",
    "                            word = self.vocab._idx_to_token[token]\n",
    "                            predicted_caption.append(word)\n",
    "\n",
    "                        # Save all predicted captions into predictions\n",
    "                        predictions.append(predicted_caption)\n",
    "                        \n",
    "                except:\n",
    "                        predictions.append([])\n",
    "                    \n",
    "                    \n",
    "        if self.test_mode == 'test':        \n",
    "            # Calculate BLEU-Scores using NLTK toolkit\n",
    "            bleu = corpus_bleu(true_captions, predictions)\n",
    "\n",
    "            self.bleu.append(bleu)\n",
    "       \n",
    "        return predictions\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60381/60381 [6:03:51<00:00,  2.77it/s]   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.TestModel at 0x1459f0e10>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TestModel.MainTest('inference_thumbnails', args.pretrained_model, 1, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
